{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "from yjlib.Data import DataLoader; \n",
    "from yjlib.prep.ImgPrep import ImgPrep \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import scipy.misc # for saving \n",
    "\n",
    "class EndoDataset(Dataset):\n",
    "    def __init__(self, DATA_PATH, IMAGE_SIZE, split):\n",
    "        dataloader = DataLoader(datatype='img')\n",
    "        self.imgPrep = ImgPrep() \n",
    "        \n",
    "        # data path\n",
    "        self.DATA_PATH = DATA_PATH\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "\n",
    "        self.split = split.upper()\n",
    "        assert self.split in {'TRAIN','VAL', 'TEST'}\n",
    "\n",
    "        # Read data files\n",
    "        with open(os.path.join(DATA_PATH, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j) # image data pathes in file\n",
    "\n",
    "        with open(os.path.join(DATA_PATH, self.split + '_labels.json'), 'r') as j:\n",
    "            self.labels = json.load(j) # boxes and labels data pathes in file\n",
    "\n",
    "        assert len(self.images) == len(self.labels) # if do not match number of images with number of boxes and labels.\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"getitem\n",
    "        Parameter\n",
    "        ---------\n",
    "        i: number of order data\n",
    "        \n",
    "        PIL(raw_image) -> array -> PIL -> Tensor(prep_image)\n",
    "        \n",
    "        \"\"\"\n",
    "        raw_image = Image.open(self.images[i], mode = 'r') # read PIL Image\n",
    "\n",
    "        # image processing\n",
    "        removed_image = self.imgPrep.remove_pad(np.array(raw_image))# convert PIL to array # remove pad\n",
    "        \n",
    "        # Transform\n",
    "        \n",
    "        \n",
    "        label = self.labels[i]\n",
    "        tensor_label = torch.FloatTensor([label])\n",
    "        \n",
    "        content_transform = transforms.Compose([transforms.Resize(self.IMAGE_SIZE),\n",
    "                                                transforms.ToTensor(), \n",
    "                                                transforms.Normalize([0,0,0],[1,1,1])])\n",
    "        \n",
    "        pil_image = Image.fromarray(removed_image) # Convert array to pil\n",
    "        prep_image = content_transform(pil_image) # transform needs PIL image\n",
    "        \n",
    "        del raw_image, removed_image, pil_image, label\n",
    "        \n",
    "        return prep_image, tensor_label\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double label and read path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 path 기반으로 dataset 및 loader 만들기\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "from yjlib.prep.ImgPrep import ImgPrep \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import scipy.misc # for saving \n",
    "\n",
    "class PathDataset(Dataset):\n",
    "    def __init__(self, IMAGE_SIZE, IMAGE_PATH=[], LABELS=[],get_path_flag=True):\n",
    "        self.imgPrep = ImgPrep() \n",
    "        \n",
    "        # data path\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "        self.images = IMAGE_PATH\n",
    "        self.labels = LABELS\n",
    "        self.get_path_flag = get_path_flag\n",
    "\n",
    "        assert len(self.images) == len(self.labels) # if do not match number of images with number of boxes and labels.\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"getitem\n",
    "        Parameter\n",
    "        ---------\n",
    "        i: number of order data\n",
    "        \n",
    "        PIL(raw_image) -> array -> PIL -> Tensor(prep_image)\n",
    "        \n",
    "        \"\"\"\n",
    "        raw_image = Image.open(self.images[i], mode = 'r') # read PIL Image\n",
    "\n",
    "        # image processing\n",
    "        removed_image = self.imgPrep.remove_pad(np.array(raw_image))# convert PIL to array # remove pad\n",
    "        \n",
    "        label = self.labels[i]\n",
    "        tensor_label = torch.FloatTensor([label])\n",
    "        \n",
    "        content_transform = transforms.Compose([transforms.Resize(self.IMAGE_SIZE),\n",
    "                                                transforms.ToTensor(), \n",
    "                                                transforms.Normalize([0,0,0],[1,1,1])])\n",
    "        \n",
    "        pil_image = Image.fromarray(removed_image) # Convert array to pil\n",
    "        prep_image = content_transform(pil_image) # transform needs PIL image\n",
    "        \n",
    "        del raw_image, removed_image, pil_image, label\n",
    "        \n",
    "        if self.get_path_flag == False:\n",
    "            return prep_image, tensor_label\n",
    "        elif self.get_path_flag == True:\n",
    "            return prep_image, tensor_label, self.images[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 path 기반으로 dataset 및 loader 만들기\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "from yjlib.prep.ImgPrep import ImgPrep \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import scipy.misc # for saving \n",
    "\n",
    "class PathDataset(Dataset):\n",
    "    def __init__(self, IMAGE_SIZE, IMAGE_PATH=[], LABELS=[],get_path_flag=True):\n",
    "        self.imgPrep = ImgPrep() \n",
    "        \n",
    "        # data path\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "        self.images = IMAGE_PATH\n",
    "        self.labels = LABELS\n",
    "        self.get_path_flag = get_path_flag\n",
    "\n",
    "        assert len(self.images) == len(self.labels) # if do not match number of images with number of boxes and labels.\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"getitem\n",
    "        Parameter\n",
    "        ---------\n",
    "        i: number of order data\n",
    "        \n",
    "        PIL(raw_image) -> array -> PIL -> Tensor(prep_image)\n",
    "        \n",
    "        \"\"\"\n",
    "        raw_image = Image.open(self.images[i], mode = 'r') # read PIL Image\n",
    "\n",
    "        # image processing\n",
    "        removed_image = self.imgPrep.remove_pad(np.array(raw_image))# convert PIL to array # remove pad\n",
    "        \n",
    "        label = self.labels[i]\n",
    "        tensor_label = torch.FloatTensor([label])\n",
    "        \n",
    "        content_transform = transforms.Compose([transforms.Resize(self.IMAGE_SIZE),\n",
    "                                                transforms.ToTensor(), \n",
    "                                                transforms.Normalize([0,0,0],[1,1,1])])\n",
    "        \n",
    "        pil_image = Image.fromarray(removed_image) # Convert array to pil\n",
    "        prep_image = content_transform(pil_image) # transform needs PIL image\n",
    "        \n",
    "        del raw_image, removed_image, pil_image, label\n",
    "        \n",
    "        if self.get_path_flag == False:\n",
    "            return prep_image, tensor_label\n",
    "        elif self.get_path_flag == True:\n",
    "            return prep_image, tensor_label, self.images[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "class EndoDataset(Dataset):\n",
    "    def __init__(self, DATA_PATH, IMAGE_SIZE, split):\n",
    "        dataloader = DataLoader(datatype='img')\n",
    "        self.imgPrep = ImgPrep() \n",
    "        \n",
    "        # data path\n",
    "        self.DATA_PATH = DATA_PATH\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "\n",
    "        self.split = split.upper()\n",
    "        assert self.split in {'TRAIN','VAL', 'TEST'}\n",
    "\n",
    "        # Read data files\n",
    "        with open(os.path.join(DATA_PATH, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j) # image data pathes in file\n",
    "\n",
    "        with open(os.path.join(DATA_PATH, self.split + '_labels.json'), 'r') as j:\n",
    "            self.labels = json.load(j) # boxes and labels data pathes in file\n",
    "\n",
    "        assert len(self.images) == len(self.labels) # if do not match number of images with number of boxes and labels.\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"getitem\n",
    "        Parameter\n",
    "        ---------\n",
    "        i: number of order data\n",
    "        \n",
    "        PIL(raw_image) -> array -> PIL -> Tensor(prep_image)\n",
    "        \n",
    "        \"\"\"\n",
    "        raw_image = Image.open(self.images[i], mode = 'r') # read PIL Image\n",
    "\n",
    "        # image processing\n",
    "        removed_image = self.imgPrep.remove_pad(np.array(raw_image))# convert PIL to array # remove pad\n",
    "        \n",
    "        # Transform\n",
    "        \n",
    "        \n",
    "        label = self.labels[i]\n",
    "        tensor_label = torch.FloatTensor([label])\n",
    "        \n",
    "        content_transform = transforms.Compose([transforms.Resize(self.IMAGE_SIZE),\n",
    "                                                transforms.ToTensor(), \n",
    "                                                transforms.Normalize([0,0,0],[1,1,1])])\n",
    "        \n",
    "        pil_image = Image.fromarray(removed_image) # Convert array to pil\n",
    "        prep_image = content_transform(pil_image) # transform needs PIL image\n",
    "        \n",
    "        del raw_image, removed_image, pil_image, label\n",
    "        \n",
    "        return prep_image, tensor_label\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced dataset loading\n",
    "https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(images, nclasses):                        \n",
    "    count = [0] * nclasses                                                      \n",
    "    for item in images:                                                         \n",
    "        count[item[1]] += 1                                                     \n",
    "    weight_per_class = [0.] * nclasses                                      \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(nclasses):                                                   \n",
    "        weight_per_class[i] = N/float(count[i])                                 \n",
    "    weight = [0] * len(images)                                              \n",
    "    for idx, val in enumerate(images):                                          \n",
    "        weight[idx] = weight_per_class[val[1]]                                  \n",
    "    return weight     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.ImageFolder(traindir)                                                                         \n",
    "                                                                                \n",
    "# For unbalanced dataset we create a weighted sampler                       \n",
    "weights = make_weights_for_balanced_classes(dataset_train.imgs, len(dataset_train.classes))                                                                \n",
    "weights = torch.DoubleTensor(weights)            \n",
    "\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))                     \n",
    "                                                                                \n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           shuffle = True,\n",
    "                                           sampler = sampler, \n",
    "                                           num_workers=args.workers, \n",
    "                                           pin_memory=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
