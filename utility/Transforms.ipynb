{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize \n",
    "# rotation\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "# from yjlib.Datasets import EndoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a parameter\n",
    "NAME_OF_CLASS = ['normal','lgd','hgd','cancer'] # This name same with name of file\n",
    "JSON_PATH = '/disk1/yjkim/json/intestine/c4json'\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "WORKERS = 2\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EndoDataset(DATA_PATH=JSON_PATH,IMAGE_SIZE = IMG_SIZE, split='train',\n",
    "                           get_path_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True, \n",
    "                                           num_workers = WORKERS, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find image mean std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ = torch.Tensor([[[1,2,3],[4,5,6],[7,8,9]],\n",
    "                     [[2,3,4],[5,6,7],[8,9,10]],\n",
    "                     [[3,4,5],[6,7,8],[9,10,11]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "6.0\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([[1,2,3],[4,5,6],[7,8,9]]))\n",
    "print(np.mean([[2,3,4],[5,6,7],[8,9,10]]))\n",
    "print(np.mean([[3,4,5],[6,7,8],[9,10,11]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  3.,  4.],\n",
       "        [ 5.,  6.,  7.],\n",
       "        [ 8.,  9., 10.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7386)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3817)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Normalize, ToTensor , ToPILImage,  functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ = [0]*3\n",
    "std_ = [1]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ = [0]*3\n",
    "std_ = [1]*3\n",
    "\n",
    "cp = Compose([ToPILImage(mode='RGB'),\n",
    "              ToTensor(),\n",
    "              Normalize(mean_ ,std_)\n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = random.randint(-30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8b645d0b70>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.],\n",
       "         [ 7.,  8.,  9.]],\n",
       "\n",
       "        [[ 2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.],\n",
       "         [ 8.,  9., 10.]],\n",
       "\n",
       "        [[ 3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.9961, 0.9922],\n",
       "         [0.9882, 0.9843, 0.9804],\n",
       "         [0.9765, 0.9725, 0.9686]],\n",
       "\n",
       "        [[0.9961, 0.9922, 0.9882],\n",
       "         [0.9843, 0.9804, 0.9765],\n",
       "         [0.9725, 0.9686, 0.9647]],\n",
       "\n",
       "        [[0.9922, 0.9882, 0.9843],\n",
       "         [0.9804, 0.9765, 0.9725],\n",
       "         [0.9686, 0.9647, 0.9608]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp (img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.misc # for saving \n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "from yjlib.prep.ImgPrep import ImgPrep \n",
    "\n",
    "\n",
    "class EndoDataset(Dataset):\n",
    "    def __init__(self, DATA_PATH, IMAGE_SIZE, split, get_path_flag=True):\n",
    "        self.imgPrep = ImgPrep() \n",
    "        \n",
    "        # data path\n",
    "        self.DATA_PATH = DATA_PATH\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "        self.get_path_flag = get_path_flag\n",
    "\n",
    "        self.split = split.upper()\n",
    "        assert self.split in {'TRAIN','VAL', 'TEST'}\n",
    "\n",
    "        # Read data files\n",
    "        with open(os.path.join(DATA_PATH, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j) # image data pathes in file\n",
    "\n",
    "        with open(os.path.join(DATA_PATH, self.split + '_labels.json'), 'r') as j:\n",
    "            self.labels = json.load(j) # boxes and labels data pathes in file\n",
    "\n",
    "        assert len(self.images) == len(self.labels) # if do not match number of images with number of boxes and labels.\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"getitem\n",
    "        Parameter\n",
    "        ---------\n",
    "        i: number of order data\n",
    "        \n",
    "        PIL(raw_image) -> array -> PIL -> Tensor(prep_image)\n",
    "        \n",
    "        \"\"\"\n",
    "        raw_image = Image.open(self.images[i], mode = 'r') # read PIL Image\n",
    "\n",
    "        # image processing\n",
    "        removed_image = self.imgPrep.remove_pad(np.array(raw_image))# convert PIL to array # remove pad\n",
    "        \n",
    "        # Transform\n",
    "        \n",
    "        \n",
    "        label = self.labels[i]\n",
    "        tensor_label = torch.FloatTensor([label])\n",
    "        \n",
    "        content_transform = Compose([Resize(self.IMAGE_SIZE),\n",
    "                                                ToTensor(),\n",
    "#                                                 Normalize([0,0,0],[1,1,1])\n",
    "                                                ])\n",
    "        \n",
    "        pil_image = Image.fromarray(removed_image) # Convert array to pil\n",
    "        prep_image = content_transform(pil_image) # transform needs PIL image\n",
    "        \n",
    "        del raw_image, removed_image, pil_image, label\n",
    "        \n",
    "        if self.get_path_flag == False:\n",
    "            return prep_image, tensor_label\n",
    "        elif self.get_path_flag == True:\n",
    "            return prep_image, tensor_label , self.images[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
